version: '3.8'

services:
  # Ollama LLM Service with GPU Support
  ollama:
    image: ollama/ollama:latest
    container_name: university_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    networks:
      - university_network
    restart: unless-stopped
    # GPU configuration for Ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Model Initialization (Qwen3)
  model_init:
    image: ollama/ollama:latest
    container_name: university_model_init
    networks:
      - university_network
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=ollama
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "echo 'Waiting for Ollama to be ready...' && \
      sleep 20 && \
      ollama pull qwen3:8b && \
      echo 'Model qwen3:8b pulled successfully' && \
      exit 0"
    restart: "no"

  # FastAPI Backend
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: university_backend
    ports:
      - "8000:8000"
    volumes:
      # Mount ALL Python files, not just backend folder
      - ./main.py:/app/main.py                    # Mount main.py
      - ./backend:/app/backend                    # Mount backend folder
      - ./data:/app/data                          # Mount data folder
      # Mount any other root Python files if they exist
      - ./requirements.txt:/app/requirements.txt
      # If you have other Python files at root, mount them too:
      # - ./config.py:/app/config.py
      # - ./utils.py:/app/utils.py
    environment:
      # Cassandra Configuration
      - CASSANDRA_HOST=sunway.hep88.com
      - CASSANDRA_PORT=9042
      - CASSANDRA_USERNAME=planusertest
      - CASSANDRA_PASSWORD=Ic7cU8K965Zqx
      - CASSANDRA_KEYSPACE=subjectplanning
      
      # Ollama/LLM Configuration
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - QWEN_MODEL=qwen3:8b  # Changed from LLAMA_MODEL
      
      # GPU Configuration
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      
      # API Configuration
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_DEBUG=true
      
      # Python Configuration
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
    networks:
      - university_network
    depends_on:
      - ollama
      - model_init
    restart: unless-stopped
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  # React Frontend
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: university_frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - GENERATE_SOURCEMAP=false
      - NODE_ENV=development
    networks:
      - university_network
    depends_on:
      - backend
    restart: unless-stopped
    stdin_open: true
    tty: true

volumes:
  ollama_data:
    driver: local

networks:
  university_network:
    driver: bridge